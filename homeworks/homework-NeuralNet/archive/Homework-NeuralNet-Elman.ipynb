{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f1b0e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start with some packages we need\n",
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f7455dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all tokens: ['<EOS>', '<PAD>', '<SOS>', 'book', 'boy', 'bread', 'break', 'car', 'cat', 'chase', 'cookie', 'dog', 'dragon', 'eat', 'exist', 'girl', 'glass', 'like', 'lion', 'man', 'monster', 'mouse', 'move', 'plate', 'rock', 'sandwich', 'see', 'sleep', 'smash', 'smell', 'think', 'woman'] \n",
      "\n",
      "mapping tokens to indices: {'<EOS>': 0, '<PAD>': 1, '<SOS>': 2, 'book': 3, 'boy': 4, 'bread': 5, 'break': 6, 'car': 7, 'cat': 8, 'chase': 9, 'cookie': 10, 'dog': 11, 'dragon': 12, 'eat': 13, 'exist': 14, 'girl': 15, 'glass': 16, 'like': 17, 'lion': 18, 'man': 19, 'monster': 20, 'mouse': 21, 'move': 22, 'plate': 23, 'rock': 24, 'sandwich': 25, 'see': 26, 'sleep': 27, 'smash': 28, 'smell': 29, 'think': 30, 'woman': 31} \n",
      "\n",
      "first training pattern: <SOS> lion eat man <EOS> \n",
      "\n",
      "tensor representation of tensor([ 2, 18, 13, 19,  0]): \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Turn a sequence of tokens into a tensor <seq_length>\n",
    "def sentenceToTensor(tokens_list):\n",
    "    assert(isinstance(tokens_list,list))\n",
    "    tokens_index = [token_to_index[token] for token in tokens_list]\n",
    "    return torch.tensor(tokens_index)\n",
    "\n",
    "def pad_sentence(tokens_list,max_len):\n",
    "    assert(isinstance(tokens_list,list))\n",
    "    npad = max_len-len(tokens_list)\n",
    "    tokens_list += npad*['<PAD>']\n",
    "    return tokens_list\n",
    "\n",
    "# load the training and test data\n",
    "with open('data/elman_sentences.txt','r') as fid:\n",
    "    lines = fid.readlines()\n",
    "\n",
    "sentences_str = [l.strip() for l in lines]\n",
    "sentences_str = ['<SOS> ' + s + ' <EOS>' for s in sentences_str]\n",
    "sentences_tokens = [s.split() for s in sentences_str]\n",
    "max_len = max(len(s) for s in sentences_tokens)\n",
    "sentences_tokens = [pad_sentence(s, max_len) for s in sentences_tokens]\n",
    "all_tokens = sorted(set(sum(sentences_tokens,[])))\n",
    "n_tokens = len(all_tokens) # total number of possible symbols\n",
    "token_to_index = {t : i for i,t in enumerate(all_tokens)}\n",
    "index_to_token = {i : t for i,t in enumerate(all_tokens)}\n",
    "training_pats = [sentenceToTensor(s) for s in sentences_tokens]\n",
    "training_pats_tensor = torch.stack(training_pats,dim=0) # 10000 x 5 tensor for all training pats\n",
    "ntrain = len(training_pats)\n",
    "print('all tokens: %s \\n' % all_tokens)\n",
    "print('mapping tokens to indices: %s \\n' % token_to_index)\n",
    "print('first training pattern: %s \\n' % sentences_str[1])\n",
    "print('tensor representation of %s: \\n' % training_pats[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bc84a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dendo(X, names, exclude=['<SOS>','<EOS>','<PAD>']):\n",
    "    #  X : numpy tensor [nitem x dim]\n",
    "    #  names : [nitem list] list of item names\n",
    "    #  exclude: list of names we want to exclude       \n",
    "    names  = np.array(names)\n",
    "    nitem = len(names)\n",
    "    inc = np.ones(nitem,dtype=bool)\n",
    "    for e in exclude:\n",
    "        if e in token_to_index:\n",
    "            inc[token_to_index[e]] = False\n",
    "#     linked = linkage(X[inc],'single',metric='cosine', optimal_ordering=True)\n",
    "    linked = linkage(X[inc],'single', optimal_ordering=True)\n",
    "    plt.figure(1, figsize=(20,6))\n",
    "    dendrogram(linked, labels=names[inc], color_threshold=0, leaf_font_size=8)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fe40b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRN(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        #  nsymbols: number of possible input/output symbols\n",
    "        super(SRN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embed = nn.Embedding(vocab_size,hidden_size)\n",
    "        self.i2h = nn.Linear(2*hidden_size, hidden_size)\n",
    "        self.h2o = nn.Linear(hidden_size, vocab_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=0)\n",
    "\n",
    "    def forward(self, input_token_index, hidden):\n",
    "        # input_token_index: [integer] index of current token\n",
    "        # hidden: [tensor of length hidden_size] previous hidden state\n",
    "        input_embed = self.embed(input_token_index) # hidden_size tensor\n",
    "        combined = torch.cat((input_embed, hidden), 0) # 2*hidden_size\n",
    "        hidden = self.i2h(combined) # hidden_size\n",
    "        hidden = torch.sigmoid(hidden)\n",
    "        output = self.h2o(hidden) # vocab_size \n",
    "        output = self.softmax(output) # vocab_size\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17211141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  1: train loss 2.03\n",
      "epoch  2: train loss 1.67\n",
      "epoch  3: train loss 1.63\n",
      "epoch  4: train loss 1.61\n",
      "epoch  5: train loss 1.59\n",
      "epoch  6: train loss 1.58\n",
      "epoch  7: train loss 1.58\n",
      "epoch  8: train loss 1.57\n",
      "epoch  9: train loss 1.57\n",
      "epoch  10: train loss 1.57\n",
      "epoch  11: train loss 1.57\n",
      "epoch  12: train loss 1.56\n",
      "epoch  13: train loss 1.56\n",
      "epoch  14: train loss 1.56\n",
      "epoch  15: train loss 1.56\n",
      "epoch  16: train loss 1.56\n",
      "epoch  17: train loss 1.56\n",
      "epoch  18: train loss 1.56\n",
      "epoch  19: train loss 1.56\n",
      "epoch  20: train loss 1.56\n",
      "epoch  21: train loss 1.56\n",
      "epoch  22: train loss 1.56\n",
      "epoch  23: train loss 1.56\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/bt/pnwcbyd10hn24vkl9b_83hkw0000gp/T/ipykernel_61528/4231504773.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0mloss_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mb_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/bt/pnwcbyd10hn24vkl9b_83hkw0000gp/T/ipykernel_61528/4231504773.py\u001b[0m in \u001b[0;36mbtrain\u001b[0;34m(batch, rnn)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/CCM/lib/python3.7/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/CCM/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/CCM/lib/python3.7/site-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    146\u001b[0m                     \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m                     \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight_decay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m                     eps=group['eps'])\n\u001b[0m\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/CCM/lib/python3.7/site-packages/torch/optim/_functional.py\u001b[0m in \u001b[0;36madamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m         \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class bSRN(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size,hidden_size)\n",
    "        self.rnn = nn.RNN(input_size=hidden_size,hidden_size=hidden_size,batch_first=True)\n",
    "        self.h2o = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, z):\n",
    "        # z: [bsize x seqlen] input tokens as indices\n",
    "        z_embed = self.embed(z) # bsize x max_len x hidden_size\n",
    "        hidden, _ = self.rnn(z_embed) \n",
    "            # output : bsize x max_len x hidden_size\n",
    "        output = self.h2o(hidden) # bsize x max_len x vocab_size\n",
    "        return output\n",
    "    \n",
    "def btrain(batch,rnn):\n",
    "    rnn.train()\n",
    "    rnn.zero_grad()\n",
    "    loss = 0\n",
    "    inputs = batch[:,:-1] # bsize x max_len-1 x vocab_size\n",
    "    targets = batch[:,1:] # bsize x max_len-1 x vocab_size\n",
    "    output = rnn(inputs) # bsize x max_len x vocab_size\n",
    "    loss = loss_fn(output.transpose(1,2),targets)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "nepochs = 100 # number of passes through the entire training set \n",
    "nhidden = 10 # number of hidden units\n",
    "rnn = bSRN(n_tokens, nhidden) # create the network\n",
    "optimizer = torch.optim.AdamW(rnn.parameters(), weight_decay=0.04)\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=token_to_index['<PAD>'])\n",
    "# loss_fn = torch.nn.CrossEntropyLoss()\n",
    "D = torch.utils.data.TensorDataset(training_pats_tensor)\n",
    "dataloader = torch.utils.data.DataLoader(D, batch_size=8, shuffle=True)\n",
    "for myiter in range(1,nepochs+1):\n",
    "    loss_total = 0.\n",
    "    b_total = 0\n",
    "    for batch in dataloader:\n",
    "        batch = batch[0]\n",
    "        loss = btrain(batch, rnn)\n",
    "        loss_total += loss\n",
    "        b_total += 1\n",
    "    if myiter % 1 ==0:\n",
    "        print(\"epoch  %s: train loss %2.2f\" % (myiter, loss_total/b_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b6b7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(seq_tensor, rnn):\n",
    "    # seq_tensor: [seq_length tensor]\n",
    "    # rnn : instance of SRN class\n",
    "    hidden = rnn.initHidden()\n",
    "    rnn.train()\n",
    "    rnn.zero_grad()\n",
    "    loss = 0\n",
    "    seq_length = seq_tensor.shape[0]\n",
    "    for i in range(seq_length-1):\n",
    "        output, hidden = rnn(seq_tensor[i], hidden)\n",
    "        loss += criterion(output, seq_tensor[i+1])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item() / float(seq_length-1)\n",
    "\n",
    "def gen(rnn,maxlen=4):\n",
    "    hidden = rnn.initHidden()\n",
    "    rnn.eval()\n",
    "    S = [token_to_index['<SOS>']]\n",
    "    for i in range(maxlen):\n",
    "        output, hidden = rnn(torch.tensor(S[-1]), hidden)\n",
    "        m = torch.distributions.categorical.Categorical(logits=output)\n",
    "        S.append(m.sample().item())\n",
    "    print(' '.join([index_to_token[s] for s in S]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c0c754",
   "metadata": {},
   "outputs": [],
   "source": [
    "nepochs = 10 # number of passes through the entire training set \n",
    "nhidden = 20 # number of hidden units\n",
    "\n",
    "rnn = SRN(n_tokens,nhidden) # create the network\n",
    "optimizer = torch.optim.AdamW(rnn.parameters(), weight_decay=0.04) # stochastic gradient descent\n",
    "# optimizer = torch.optim.Adam(rnn.parameters()) # stochastic gradient descent\n",
    "criterion = nn.NLLLoss() #log-likelihood loss function\n",
    "\n",
    "for myiter in range(1,nepochs+1): # for each epoch\n",
    "    permute = np.random.permutation(ntrain)\n",
    "    loss = np.zeros(ntrain)\n",
    "    for p in permute:\n",
    "        pat = training_pats[p]\n",
    "        loss[p] = train(pat, rnn)\n",
    "    if myiter % 1 ==0:\n",
    "        print(\"epoch  %s: train loss %2.2f\" % (myiter, np.mean(loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff077cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen(rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1355c0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dendo(rnn.embed(torch.arange(n_tokens)).detach().numpy(),all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1375845c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dendo(rnn.embed(torch.arange(n_tokens)).detach().numpy(),all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5e3ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dendo(rnn.embed(torch.arange(n_tokens)).detach().numpy(),all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ba9304",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
